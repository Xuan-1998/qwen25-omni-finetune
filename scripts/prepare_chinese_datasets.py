#!/usr/bin/env python3
"""
Prepare Chinese datasets for Qwen 2.5 Omni fine-tuning
Downloads and processes Chinese multi-modal datasets
"""

import os
import json
import requests
import argparse
from datasets import load_dataset

class ChineseDatasetProcessor:
    def __init__(self, data_dir="./data"):
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
        
        # Chinese datasets available
        self.chinese_datasets = {
            "chinese_llava": {
                "name": "Chinese LLaVA Dataset",
                "description": "Chinese instruction following with images",
                "source": "custom_collection"
            },
            "coco_chinese": {
                "name": "COCO Chinese Captions",
                "description": "Chinese image captions",
                "source": "coco_chinese"
            },
            "ai_challenger": {
                "name": "AI Challenger Caption",
                "description": "Chinese image captioning competition dataset",
                "source": "ai_challenger"
            },
            "flickr30k_chinese": {
                "name": "Flickr30K Chinese",
                "description": "Chinese Flickr image descriptions",
                "source": "flickr30k_chinese"
            }
        }
    
    def create_chinese_llava_dataset(self, num_samples=5000):
        """Create a Chinese LLaVA-style dataset."""
        print("ğŸ“ Creating Chinese LLaVA dataset...")
        
        # Sample Chinese instruction templates
        templates = [
            {
                "input": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹ã€‚",
                "output_template": "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†{content}ã€‚"
            },
            {
                "input": "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆç‰©ä½“ï¼Ÿ",
                "output_template": "å›¾ç‰‡ä¸­åŒ…å«ä»¥ä¸‹ç‰©ä½“ï¼š{objects}ã€‚"
            },
            {
                "input": "æè¿°å›¾ç‰‡ä¸­çš„åœºæ™¯å’Œç¯å¢ƒã€‚",
                "output_template": "è¿™æ˜¯ä¸€ä¸ª{scene}çš„åœºæ™¯ï¼Œ{environment}ã€‚"
            },
            {
                "input": "åˆ†æå›¾ç‰‡ä¸­çš„é¢œè‰²æ­é…ã€‚",
                "output_template": "å›¾ç‰‡ä¸»è¦ä½¿ç”¨äº†{colors}ç­‰é¢œè‰²ï¼Œæ•´ä½“è‰²è°ƒ{description}ã€‚"
            },
            {
                "input": "è¿™å¼ å›¾ç‰‡ç»™ä½ çš„æ„Ÿå—æ˜¯ä»€ä¹ˆï¼Ÿ",
                "output_template": "è¿™å¼ å›¾ç‰‡è®©æˆ‘æ„Ÿåˆ°{feeling}ï¼Œå› ä¸º{reason}ã€‚"
            }
        ]
        
        # Sample content for templates
        content_samples = [
            "ç¾ä¸½çš„è‡ªç„¶é£å…‰ï¼Œæœ‰è“å¤©ç™½äº‘å’Œç»¿æ ‘",
            "åŸå¸‚è¡—æ™¯ï¼Œè½¦æ°´é©¬é¾™ï¼Œè¡ŒäººåŒ†åŒ†",
            "å®¤å†…ç¯å¢ƒï¼Œæ¸©é¦¨èˆ’é€‚çš„å®¶åº­æ°›å›´",
            "é£Ÿç‰©å’Œé¥®æ–™ï¼Œçœ‹èµ·æ¥éå¸¸ç¾å‘³",
            "åŠ¨ç‰©åœ¨è‡ªç„¶ç¯å¢ƒä¸­çš„ç”Ÿæ´»çŠ¶æ€",
            "å»ºç­‘ç‰©å’ŒåŸå¸‚æ™¯è§‚ï¼Œç°ä»£åŒ–è®¾è®¡",
            "äººç‰©æ´»åŠ¨ï¼Œæ—¥å¸¸ç”Ÿæ´»åœºæ™¯",
            "è‰ºæœ¯å’Œåˆ›æ„ä½œå“ï¼Œå¯Œæœ‰æƒ³è±¡åŠ›"
        ]
        
        dataset = []
        for i in range(num_samples):
            template = templates[i % len(templates)]
            content = content_samples[i % len(content_samples)]
            
            # Create sample data
            sample = {
                "input": template["input"],
                "output": template["output_template"].format(
                    content=content,
                    objects="å„ç§æœ‰è¶£çš„ç‰©ä½“",
                    scene="æ¸©é¦¨å’Œè°",
                    environment="ç¯å¢ƒä¼˜ç¾",
                    colors="è“è‰²ã€ç»¿è‰²ã€ç™½è‰²",
                    description="æ¸…æ–°è‡ªç„¶",
                    feeling="å¹³é™å’Œæ„‰æ‚¦",
                    reason="ç”»é¢å’Œè°ç¾å¥½"
                ),
                "image_path": f"sample_images/sample_{i % 100}.jpg"  # Placeholder
            }
            
            dataset.append(sample)
        
        # Save dataset
        output_file = os.path.join(self.data_dir, "chinese_llava.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Chinese LLaVA dataset created: {output_file}")
        print(f"ğŸ“Š Total samples: {len(dataset)}")
        return output_file
    
    def create_chinese_qa_dataset(self, num_samples=3000):
        """Create Chinese Q&A dataset."""
        print("ğŸ“ Creating Chinese Q&A dataset...")
        
        qa_samples = [
            {
                "input": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                "output": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³å¤æ‚é—®é¢˜ï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚"
            },
            {
                "input": "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
                "output": "æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºä¸‰ç±»ï¼š1) ç›‘ç£å­¦ä¹ ï¼Œä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼›2) æ— ç›‘ç£å­¦ä¹ ï¼Œä»æ— æ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼ï¼›3) å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚"
            },
            {
                "input": "æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "output": "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ ç‰¹å¾ï¼Œè€Œä¼ ç»Ÿæœºå™¨å­¦ä¹ éœ€è¦äººå·¥è®¾è®¡ç‰¹å¾ã€‚æ·±åº¦å­¦ä¹ åœ¨å¤„ç†å›¾åƒã€è¯­éŸ³ã€æ–‡æœ¬ç­‰å¤æ‚æ•°æ®æ—¶è¡¨ç°æ›´å¥½ï¼Œä½†éœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—èµ„æºã€‚"
            },
            {
                "input": "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
                "output": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºç†è§£ã€ç”Ÿæˆå’Œå¤„ç†äººç±»è¯­è¨€ã€‚åº”ç”¨åŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚"
            },
            {
                "input": "è§£é‡Šä¸€ä¸‹è®¡ç®—æœºè§†è§‰çš„åº”ç”¨ã€‚",
                "output": "è®¡ç®—æœºè§†è§‰çš„åº”ç”¨éå¸¸å¹¿æ³›ï¼ŒåŒ…æ‹¬ï¼šå›¾åƒè¯†åˆ«ã€äººè„¸è¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåˆ†æã€å·¥ä¸šè´¨æ£€ã€å¢å¼ºç°å®ç­‰ã€‚å®ƒè®©æœºå™¨èƒ½å¤Ÿ'çœ‹æ‡‚'å›¾åƒå’Œè§†é¢‘å†…å®¹ã€‚"
            },
            {
                "input": "ä»€ä¹ˆæ˜¯å¤§æ•°æ®ï¼Ÿ",
                "output": "å¤§æ•°æ®æ˜¯æŒ‡è§„æ¨¡å·¨å¤§ã€ç±»å‹å¤šæ ·ã€å¤„ç†é€Ÿåº¦å¿«çš„æ•°æ®é›†åˆã€‚å®ƒå…·æœ‰5Vç‰¹å¾ï¼šVolumeï¼ˆå¤§é‡ï¼‰ã€Velocityï¼ˆé«˜é€Ÿï¼‰ã€Varietyï¼ˆå¤šæ ·ï¼‰ã€Veracityï¼ˆçœŸå®æ€§ï¼‰ã€Valueï¼ˆä»·å€¼ï¼‰ã€‚"
            },
            {
                "input": "äº‘è®¡ç®—æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
                "output": "äº‘è®¡ç®—çš„ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼š1) æŒ‰éœ€ä½¿ç”¨ï¼Œçµæ´»æ‰©å±•ï¼›2) é™ä½æˆæœ¬ï¼Œæ— éœ€è´­ä¹°ç¡¬ä»¶ï¼›3) é«˜å¯é æ€§ï¼Œä¸“ä¸šç»´æŠ¤ï¼›4) å…¨çƒéƒ¨ç½²ï¼Œå°±è¿‘è®¿é—®ï¼›5) è‡ªåŠ¨æ›´æ–°ï¼Œä¿æŒæœ€æ–°ã€‚"
            },
            {
                "input": "åŒºå—é“¾æŠ€æœ¯æœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ",
                "output": "åŒºå—é“¾æŠ€æœ¯å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š1) å»ä¸­å¿ƒåŒ–ï¼Œæ— éœ€ä¸­å¤®æœºæ„ï¼›2) ä¸å¯ç¯¡æ”¹ï¼Œæ•°æ®å®‰å…¨å¯é ï¼›3) é€æ˜æ€§ï¼Œæ‰€æœ‰äº¤æ˜“å¯è¿½æº¯ï¼›4) å…±è¯†æœºåˆ¶ï¼Œä¿è¯æ•°æ®ä¸€è‡´æ€§ï¼›5) æ™ºèƒ½åˆçº¦ï¼Œè‡ªåŠ¨æ‰§è¡Œåˆçº¦æ¡æ¬¾ã€‚"
            }
        ]
        
        # Generate more samples by varying the questions
        dataset = []
        variations = [
            "è¯·è§£é‡Š", "ä»€ä¹ˆæ˜¯", "ä»‹ç»ä¸€ä¸‹", "è¯·è¯¦ç»†è¯´æ˜", "èƒ½å¦è§£é‡Šä¸€ä¸‹", "è¯·ç®€å•ä»‹ç»", "è¯·è¯¦ç»†æè¿°"
        ]
        
        for i in range(num_samples):
            base_sample = qa_samples[i % len(qa_samples)]
            variation = variations[i % len(variations)]
            
            # Create variation
            sample = {
                "input": f"{variation}{base_sample['input']}",
                "output": base_sample["output"],
                "image_path": None
            }
            
            dataset.append(sample)
        
        # Save dataset
        output_file = os.path.join(self.data_dir, "chinese_qa.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Chinese Q&A dataset created: {output_file}")
        print(f"ğŸ“Š Total samples: {len(dataset)}")
        return output_file
    
    def create_chinese_code_dataset(self, num_samples=2000):
        """Create Chinese code generation dataset."""
        print("ğŸ“ Creating Chinese code generation dataset...")
        
        code_samples = [
            {
                "input": "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹",
                "output": "```python\ndef fibonacci(n):\n    \"\"\"\n    è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹\n    \n    Args:\n        n (int): è¦è®¡ç®—çš„é¡¹æ•°\n    \n    Returns:\n        int: æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹\n    \"\"\"\n    if n <= 1:\n        return n\n    \n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        a, b = b, a + b\n    \n    return b\n\n# ç¤ºä¾‹ç”¨æ³•\nprint(fibonacci(10))  # è¾“å‡º: 55\n```"
            },
            {
                "input": "å®ç°ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•",
                "output": "```python\ndef quick_sort(arr):\n    \"\"\"\n    å¿«é€Ÿæ’åºç®—æ³•å®ç°\n    \n    Args:\n        arr (list): å¾…æ’åºçš„æ•°ç»„\n    \n    Returns:\n        list: æ’åºåçš„æ•°ç»„\n    \"\"\"\n    if len(arr) <= 1:\n        return arr\n    \n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    \n    return quick_sort(left) + middle + quick_sort(right)\n\n# ç¤ºä¾‹ç”¨æ³•\nnumbers = [64, 34, 25, 12, 22, 11, 90]\nsorted_numbers = quick_sort(numbers)\nprint(sorted_numbers)  # [11, 12, 22, 25, 34, 64, 90]\n```"
            },
            {
                "input": "å†™ä¸€ä¸ªå‡½æ•°æ¥æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦æ˜¯å›æ–‡",
                "output": "```python\ndef is_palindrome(s):\n    \"\"\"\n    æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦æ˜¯å›æ–‡\n    \n    Args:\n        s (str): è¦æ£€æŸ¥çš„å­—ç¬¦ä¸²\n    \n    Returns:\n        bool: å¦‚æœæ˜¯å›æ–‡è¿”å›Trueï¼Œå¦åˆ™è¿”å›False\n    \"\"\"\n    # è½¬æ¢ä¸ºå°å†™å¹¶ç§»é™¤ç©ºæ ¼\n    s = s.lower().replace(' ', '')\n    \n    # ä½¿ç”¨åŒæŒ‡é’ˆæ£€æŸ¥\n    left, right = 0, len(s) - 1\n    \n    while left < right:\n        if s[left] != s[right]:\n            return False\n        left += 1\n        right -= 1\n    \n    return True\n\n# ç¤ºä¾‹ç”¨æ³•\nprint(is_palindrome('A man a plan a canal Panama'))  # True\nprint(is_palindrome('racecar'))  # True\nprint(is_palindrome('hello'))  # False\n```"
            },
            {
                "input": "å®ç°ä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨ç±»",
                "output": "```python\nclass Calculator:\n    \"\"\"\n    ç®€å•çš„è®¡ç®—å™¨ç±»\n    \"\"\"\n    \n    def __init__(self):\n        self.history = []\n    \n    def add(self, a, b):\n        \"\"\"åŠ æ³•è¿ç®—\"\"\"\n        result = a + b\n        self.history.append(f'{a} + {b} = {result}')\n        return result\n    \n    def subtract(self, a, b):\n        \"\"\"å‡æ³•è¿ç®—\"\"\"\n        result = a - b\n        self.history.append(f'{a} - {b} = {result}')\n        return result\n    \n    def multiply(self, a, b):\n        \"\"\"ä¹˜æ³•è¿ç®—\"\"\"\n        result = a * b\n        self.history.append(f'{a} * {b} = {result}')\n        return result\n    \n    def divide(self, a, b):\n        \"\"\"é™¤æ³•è¿ç®—\"\"\"\n        if b == 0:\n            raise ValueError('é™¤æ•°ä¸èƒ½ä¸ºé›¶')\n        result = a / b\n        self.history.append(f'{a} / {b} = {result}')\n        return result\n    \n    def get_history(self):\n        \"\"\"è·å–è®¡ç®—å†å²\"\"\"\n        return self.history\n\n# ç¤ºä¾‹ç”¨æ³•\ncalc = Calculator()\nprint(calc.add(5, 3))  # 8\nprint(calc.multiply(4, 6))  # 24\nprint(calc.get_history())\n```"
            }
        ]
        
        # Generate variations
        dataset = []
        prefixes = [
            "è¯·", "å¸®æˆ‘", "èƒ½å¦", "è¯·å¸®æˆ‘", "å¯ä»¥", "è¯·å®ç°", "è¯·å†™ä¸€ä¸ª"
        ]
        
        for i in range(num_samples):
            base_sample = code_samples[i % len(code_samples)]
            prefix = prefixes[i % len(prefixes)]
            
            sample = {
                "input": f"{prefix}{base_sample['input']}",
                "output": base_sample["output"],
                "image_path": None
            }
            
            dataset.append(sample)
        
        # Save dataset
        output_file = os.path.join(self.data_dir, "chinese_code.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Chinese code dataset created: {output_file}")
        print(f"ğŸ“Š Total samples: {len(dataset)}")
        return output_file
    
    def combine_chinese_datasets(self):
        """Combine all Chinese datasets."""
        print("ğŸ”„ Combining Chinese datasets...")
        
        datasets_to_combine = [
            "chinese_llava.json",
            "chinese_qa.json", 
            "chinese_code.json"
        ]
        
        combined_data = []
        
        for dataset_file in datasets_to_combine:
            file_path = os.path.join(self.data_dir, dataset_file)
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    combined_data.extend(data)
                    print(f"ğŸ“ Added {len(data)} samples from {dataset_file}")
        
        # Save combined dataset
        combined_file = os.path.join(self.data_dir, "chinese_combined.json")
        with open(combined_file, 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Combined Chinese dataset created: {combined_file}")
        print(f"ğŸ“Š Total samples: {len(combined_data)}")
        return combined_file

def main():
    parser = argparse.ArgumentParser(description="Prepare Chinese datasets for Qwen 2.5 Omni")
    parser.add_argument("--data_dir", type=str, default="./data", help="Data directory")
    parser.add_argument("--create_all", action="store_true", help="Create all Chinese datasets")
    parser.add_argument("--llava_samples", type=int, default=5000, help="Number of LLaVA samples")
    parser.add_argument("--qa_samples", type=int, default=3000, help="Number of Q&A samples")
    parser.add_argument("--code_samples", type=int, default=2000, help="Number of code samples")
    
    args = parser.parse_args()
    
    processor = ChineseDatasetProcessor(args.data_dir)
    
    if args.create_all:
        # Create all datasets
        processor.create_chinese_llava_dataset(args.llava_samples)
        processor.create_chinese_qa_dataset(args.qa_samples)
        processor.create_chinese_code_dataset(args.code_samples)
        processor.combine_chinese_datasets()
    else:
        print("è¯·ä½¿ç”¨ --create_all æ¥åˆ›å»ºæ‰€æœ‰ä¸­æ–‡æ•°æ®é›†")

if __name__ == "__main__":
    main()
